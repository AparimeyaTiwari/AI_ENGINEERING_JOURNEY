{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d370bc2a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a18dfcba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to my activation function repository\n",
      "\n",
      "============================================================\n",
      "  ACTIVATION FUNCTIONS TOOLKIT\n",
      "============================================================\n",
      "\n",
      "  Choose an activation function to explore:\n",
      "\n",
      "  1. Sigmoid\n",
      "  2. Tanh (Hyperbolic Tangent)\n",
      "  3. ReLU (Rectified Linear Unit)\n",
      "  4. Leaky ReLU\n",
      "  5. ELU (Exponential Linear Unit)\n",
      "  6. Softplus\n",
      "  7. Exit\n",
      "\n",
      "============================================================\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def activation(x,n):\n",
    "    '''\n",
    "    Apply an activation function element-wise.\n",
    "    \n",
    "    Args:\n",
    "        x: numpy array of any shape (raw neuron outputs)\n",
    "    \n",
    "    Returns:\n",
    "        numpy array of same shape (activated outputs)\n",
    "    \n",
    "    Requirements:\n",
    "        - Must be non-linear (not just returning x)\n",
    "        - Must work on arrays of any shape\n",
    "        - Must be deterministic\n",
    "    '''\n",
    "    # TODO: Implement your activation function\n",
    "    if n == 1:\n",
    "        z = 1/(1+np.exp(-x)) #Sigmoid \n",
    "    elif n == 2:\n",
    "        z = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x)) #Tanh\n",
    "    elif n == 3:\n",
    "        z = np.maximum(0,x) #ReLu\n",
    "    elif n == 4:\n",
    "        alpha = 0.001\n",
    "        z = np.where(x>0,x,alpha*x) #Leaky ReLu\n",
    "    elif n == 5:\n",
    "        alpha = 0.001\n",
    "        z = np.where(x>0,x,alpha*(np.exp(x)-1)) #ELU\n",
    "    elif n == 6:\n",
    "        z = np.log(1+np.exp(x)) #Softplus\n",
    "    elif n == 7:\n",
    "        return None\n",
    "    \n",
    "    result = z # Replace with your activation\n",
    "    \n",
    "    return result\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    This is the sigmoid function\n",
    "    1) Range -> 0 to 1\n",
    "    2) commonly used\n",
    "    3) vanishing gradients\n",
    "    4) not centered around 0\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    z = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    This is tanh activation function\n",
    "    1) range -> -1 to 1\n",
    "    2) zero centered\n",
    "    3) vanishing gradients \n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    ReLU(x)=max(0,x)\n",
    "    z = np.maximum(0,x)\n",
    "    This is ReLu activation function\n",
    "    1) Range -> 0 to inf\n",
    "    2) fast and simple\n",
    "    3) not 0 centered\n",
    "    4) can have dead neurons -> ones that ouput zero always\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    LeakyReLU(x)={x, x>0\n",
    "                  Î±x, x<=0}\n",
    "\n",
    "    alpha = 0.01\n",
    "    z = np.where(x>0,x,alpha*x)\n",
    "\n",
    "    This is Leakly RelU\n",
    "    1) Addresses dead neuron issue (since we get some value even for >= 0 cases)\n",
    "    2) small slope for negative values\n",
    "\n",
    "    '''\n",
    "\n",
    "    \n",
    "    '''\n",
    "    alpha = 0.01\n",
    "    z = np.where(x>0,x,alpha*(np.exp(x)-1))\n",
    "    This is Exponential Linear Unit\n",
    "    1) Smoother version of Leaky ReLu\n",
    "    2) Close 2 zero centered outputs\n",
    "    '''\n",
    "\n",
    "    '''\n",
    "    z = np.log(1+np.exp(x))\n",
    "    This is Sofplus\n",
    "    1) Smooth approximation of ReLu\n",
    "    2) Always positive\n",
    "    3) Always has a gradient\n",
    "    '''\n",
    "    \n",
    "   \n",
    "   \n",
    "\n",
    "def main():\n",
    "    print(\"Welcome to my activation function repository\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"  ACTIVATION FUNCTIONS TOOLKIT\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"\\n  Choose an activation function to explore:\\n\")\n",
    "    print(\"  1. Sigmoid\")\n",
    "    print(\"  2. Tanh (Hyperbolic Tangent)\")\n",
    "    print(\"  3. ReLU (Rectified Linear Unit)\")\n",
    "    print(\"  4. Leaky ReLU\")\n",
    "    print(\"  5. ELU (Exponential Linear Unit)\")\n",
    "    print(\"  6. Softplus\")\n",
    "    print(\"  7. Exit\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    \n",
    "    choice = int(input(\"Enter function of choice 1-8\"))\n",
    "    number = int(input(\"Enter number\"))\n",
    "    result = activation(number,choice)\n",
    "    if result:\n",
    "        print(result)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3171b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed37c26c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
